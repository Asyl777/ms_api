import requests
from bs4 import BeautifulSoup, Tag
import psycopg2
from psycopg2.extras import RealDictCursor
import os
from dotenv import load_dotenv

load_dotenv()


def get_db_connection():
    return psycopg2.connect(
        dsn=os.getenv("DATABASE_URL"),
        cursor_factory=RealDictCursor
    )


def contains_partner_info(tag: Tag) -> bool:
    if tag.name == "p":
        strong = tag.find("strong")
        return strong and "–ò–ù–§–û–†–ú–ê–¶–ò–Ø –ü–ê–†–¢–ù–ï–†–û–í" in strong.get_text(strip=True)
    return False


def clean_article_content(article: Tag) -> Tag:
    # –£–¥–∞–ª—è–µ–º –±–ª–æ–∫–∏ ATTENTION –∏ ATTACHMENTS
    for tag in article.find_all(attrs={"data-section-name": ["ATTENTION", "ATTACHMENTS"]}):
        tag.decompose()

    # –£–¥–∞–ª—è–µ–º –ø–∞—Ä—Ç–Ω–µ—Ä—Å–∫–∏–π —Ä–µ–∫–ª–∞–º–Ω—ã–π –±–ª–æ–∫
    start_tag = None
    for tag in article.find_all("p"):
        if contains_partner_info(tag):
            start_tag = tag
            break

    if start_tag:
        current = start_tag
        while current:
            next_tag = current.find_next_sibling()
            current.decompose()
            if (isinstance(next_tag, Tag) and
                    next_tag.name == "div" and
                    "---------------------------------------------------------" in next_tag.get_text()):
                next_tag.decompose()
                break
            current = next_tag

    return article


def parse_and_save_article(article_id: int, url: str, cursor) -> None:
    try:
        response = requests.get(url, headers={"User-Agent": "Mozilla/5.0"})
        response.encoding = 'utf-8'
        soup = BeautifulSoup(response.text, "html.parser")

        article = soup.find("article")
        if not article:
            print(f"‚ùå –°—Ç–∞—Ç—å—è {article_id}: –Ω–µ –Ω–∞–π–¥–µ–Ω —Ç–µ–≥ article")
            return

        # –û—á–∏—â–∞–µ–º –∫–æ–Ω—Ç–µ–Ω—Ç
        article = clean_article_content(article)

        # –ù–∞—Ö–æ–¥–∏–º –≤—Å–µ —Ä–∞–∑–¥–µ–ª—ã
        sections = article.find_all("section", class_="page-section")

        # –£–¥–∞–ª—è–µ–º —Å—Ç–∞—Ä—ã–µ —Ä–∞–∑–¥–µ–ª—ã
        cursor.execute(
            "DELETE FROM article_sections WHERE article_id = %s",
            (article_id,)
        )

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –Ω–æ–≤—ã–µ —Ä–∞–∑–¥–µ–ª—ã
        for section in sections:
            title = section.find("h2", class_="page-section__title")
            title_text = title.get_text(strip=True) if title else "–ë–µ–∑ –Ω–∞–∑–≤–∞–Ω–∏—è"

            # –î–µ–ª–∞–µ–º –≤—Å–µ —Å—Å—ã–ª–∫–∏ –Ω–µ–∫–ª–∏–∫–∞–±–µ–ª—å–Ω—ã–º–∏
            for a_tag in section.find_all("a"):
                if a_tag.has_attr("href"):
                    del a_tag["href"]

            html_content = str(section)

            cursor.execute("""
                           INSERT INTO article_sections (article_id, section_title, html_content)
                           VALUES (%s, %s, %s)
                           """, (article_id, title_text, html_content))

        print(f"‚úÖ –°—Ç–∞—Ç—å—è {article_id}: –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ {len(sections)} —Ä–∞–∑–¥–µ–ª–æ–≤")

    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ —Å—Ç–∞—Ç—å–∏ {article_id}: {str(e)}")
        raise


def main():
    conn = get_db_connection()
    cur = conn.cursor()

    try:
        # –ü–æ–ª—É—á–∞–µ–º —Å—Ç–∞—Ç—å–∏ –∏–∑ –±–∞–∑—ã
        cur.execute("SELECT id, url FROM articles WHERE url IS NOT NULL")
        articles = cur.fetchall()

        for article in articles:
            print(f"üìÑ –û–±—Ä–∞–±–æ—Ç–∫–∞ —Å—Ç–∞—Ç—å–∏ {article['id']}...")
            parse_and_save_article(article['id'], article['url'], cur)
            conn.commit()

        print("\n‚úÖ –ü–∞—Ä—Å–∏–Ω–≥ –∑–∞–≤–µ—Ä—à–µ–Ω —É—Å–ø–µ—à–Ω–æ")

    except Exception as e:
        print(f"\n‚ùå –û—à–∏–±–∫–∞: {str(e)}")
        conn.rollback()
    finally:
        cur.close()
        conn.close()


if __name__ == "__main__":
    main()